{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kevinzakka/clip_playground/blob/main/CLIP_GradCAM_Visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpPbEKLBmyX1"
   },
   "source": [
    "# CLIP GradCAM Colab\n",
    "\n",
    "This Colab notebook uses [GradCAM](https://arxiv.org/abs/1610.02391) on OpenAI's [CLIP](https://openai.com/blog/clip/) model to produce a heatmap highlighting which regions in an image activate the most to a given caption.\n",
    "\n",
    "**Note:** Currently only works with the ResNet variants of CLIP. ViT support coming soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "form",
    "id": "qQOvOhnKQ-Tu"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import clip\n",
    "from PIL import Image\n",
    "from scipy.ndimage import filters\n",
    "from torch import nn\n",
    "from torch.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "caPbAhFlRBwT"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "\n",
    "#@markdown Some helper functions for overlaying heatmaps on top\n",
    "#@markdown of images and visualizing with matplotlib.\n",
    "\n",
    "def normalize(x: np.ndarray) -> np.ndarray:\n",
    "    # Normalize to [0, 1].\n",
    "    x = x - x.min()\n",
    "    if x.max() > 0:\n",
    "        x = x / x.max()\n",
    "    return x\n",
    "\n",
    "# Modified from: https://github.com/salesforce/ALBEF/blob/main/visualization.ipynb\n",
    "def getAttMap(img, attn_map, blur=True):\n",
    "    if blur:\n",
    "        attn_map = filters.gaussian_filter(attn_map, 0.02*max(img.shape[:2]))\n",
    "    attn_map = normalize(attn_map)\n",
    "    cmap = plt.get_cmap('jet')\n",
    "    attn_map_c = np.delete(cmap(attn_map), 3, 2)\n",
    "    attn_map = 1*(1-attn_map**0.7).reshape(attn_map.shape + (1,))*img + \\\n",
    "            (attn_map**0.7).reshape(attn_map.shape+(1,)) * attn_map_c\n",
    "    return attn_map\n",
    "\n",
    "def viz_attn(img, attn_map, blur=True):\n",
    "    _, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(img)\n",
    "    axes[1].imshow(getAttMap(img, attn_map, blur))\n",
    "    for ax in axes:\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "def load_image(img_path, resize=None):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    if resize is not None:\n",
    "        image = image.resize((resize, resize))\n",
    "    return np.asarray(image).astype(np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cellView": "form",
    "id": "XziodsCqVC2A"
   },
   "outputs": [],
   "source": [
    "class Hook:\n",
    "    \"\"\"Attaches to a module and records its activations and gradients.\"\"\"\n",
    "\n",
    "    def __init__(self, module: nn.Module):\n",
    "        self.data = None\n",
    "        self.hook = module.register_forward_hook(self.save_grad)\n",
    "        \n",
    "    def save_grad(self, module, input, output):\n",
    "        self.data = output\n",
    "        output.requires_grad_(True)\n",
    "        output.retain_grad()\n",
    "        \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        self.hook.remove()\n",
    "        \n",
    "    @property\n",
    "    def activation(self) -> torch.Tensor:\n",
    "        return self.data\n",
    "    \n",
    "    @property\n",
    "    def gradient(self) -> torch.Tensor:\n",
    "        return self.data.grad\n",
    "\n",
    "\n",
    "# Reference: https://arxiv.org/abs/1610.02391\n",
    "def gradCAM(\n",
    "    model: nn.Module,\n",
    "    input: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    layer: nn.Module\n",
    ") -> torch.Tensor:\n",
    "    # Zero out any gradients at the input.\n",
    "    if input.grad is not None:\n",
    "        input.grad.data.zero_()\n",
    "        \n",
    "    # Disable gradient settings.\n",
    "    requires_grad = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        requires_grad[name] = param.requires_grad\n",
    "        param.requires_grad_(False)\n",
    "        \n",
    "    # Attach a hook to the model at the desired layer.\n",
    "    assert isinstance(layer, nn.Module)\n",
    "    with Hook(layer) as hook:        \n",
    "        # Do a forward and backward pass.\n",
    "        with autocast('cuda'):\n",
    "            output = model.forward(input)\n",
    "        output.backward(target)\n",
    "\n",
    "        grad = hook.gradient.float()\n",
    "        act = hook.activation.float()\n",
    "    \n",
    "        # Global average pool gradient across spatial dimension\n",
    "        # to obtain importance weights.\n",
    "        alpha = grad.mean(dim=(2, 3), keepdim=True)\n",
    "        # Weighted combination of activation maps over channel\n",
    "        # dimension.\n",
    "        gradcam = torch.sum(act * alpha, dim=1, keepdim=True)\n",
    "        # We only want neurons with positive influence so we\n",
    "        # clamp any negative ones.\n",
    "        gradcam = torch.clamp(gradcam, min=0)\n",
    "\n",
    "    # Resize gradcam to input resolution.\n",
    "    gradcam = F.interpolate(\n",
    "        gradcam,\n",
    "        input.shape[2:],\n",
    "        mode='bicubic',\n",
    "        align_corners=False)\n",
    "    \n",
    "    # Restore gradient settings.\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad_(requires_grad[name])\n",
    "        \n",
    "    return gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "_bSzlC60WVkQ",
    "outputId": "f3781f8a-1993-49d0-ace1-a545beec61c3"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-3, 2], but got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m image_np \u001b[38;5;241m=\u001b[39m load_image(image_path, model\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39minput_resolution)\n\u001b[1;32m     25\u001b[0m text_input \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mtokenize([image_caption])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 27\u001b[0m attn_map \u001b[38;5;241m=\u001b[39m \u001b[43mgradCAM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_layer\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m attn_map \u001b[38;5;241m=\u001b[39m attn_map\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     35\u001b[0m viz_attn(image_np, attn_map, blur)\n",
      "Cell \u001b[0;32mIn[34], line 58\u001b[0m, in \u001b[0;36mgradCAM\u001b[0;34m(model, input, target, layer)\u001b[0m\n\u001b[1;32m     54\u001b[0m act \u001b[38;5;241m=\u001b[39m hook\u001b[38;5;241m.\u001b[39mactivation\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Global average pool gradient across spatial dimension\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# to obtain importance weights.\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Weighted combination of activation maps over channel\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# dimension.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m gradcam \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(act \u001b[38;5;241m*\u001b[39m alpha, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-3, 2], but got 3)"
     ]
    }
   ],
   "source": [
    "#@title Run\n",
    "\n",
    "#@markdown #### Image & Caption settings\n",
    "image_url = 'https://images2.minutemediacdn.com/image/upload/c_crop,h_706,w_1256,x_0,y_64/f_auto,q_auto,w_1100/v1554995050/shape/mentalfloss/516438-istock-637689912.jpg' #@param {type:\"string\"}\n",
    "\n",
    "image_caption = 'the cat' #@param {type:\"string\"}\n",
    "#@markdown ---\n",
    "#@markdown #### CLIP model settings\n",
    "clip_model = \"ViT-B/32\" #@param [\"RN50\", \"RN101\", \"RN50x4\", \"RN50x16\"]\n",
    "saliency_layer = \"layer4\" #@param [\"layer4\", \"layer3\", \"layer2\", \"layer1\"]\n",
    "#@markdown ---\n",
    "#@markdown #### Visualization settings\n",
    "blur = True #@param {type:\"boolean\"}\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(clip_model, device=device, jit=False)\n",
    "target_layer = model.visual.transformer.resblocks[-1].ln_1\n",
    "\n",
    "# Download the image from the web.\n",
    "image_path = 'image.png'\n",
    "urllib.request.urlretrieve(image_url, image_path)\n",
    "\n",
    "image_input = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "image_np = load_image(image_path, model.visual.input_resolution)\n",
    "text_input = clip.tokenize([image_caption]).to(device)\n",
    "\n",
    "attn_map = gradCAM(\n",
    "    model.visual,\n",
    "    image_input,\n",
    "    model.encode_text(text_input),\n",
    "    target_layer\n",
    ")\n",
    "attn_map = attn_map.squeeze().detach().cpu().numpy()\n",
    "\n",
    "viz_attn(image_np, attn_map, blur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOFgJrUbydCsBVszpKX1bxs",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "CLIP GradCAM Visualization ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "clipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
